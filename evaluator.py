from openai import OpenAI
import json
from collections import defaultdict

class DialogueEvaluator:
    def __init__(self, api_key, base_url):
        self.api_key = api_key
        self.base_url = base_url
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
        # Define the three evaluation prompts
        self.prompt1 = """Role:
You are an expert evaluator for task-oriented dialogue systems, responsible for assessing the completion and relevance of dialogues generated by the system.

Task:
Evaluate the following dialogue to determine the Success Rate (SR) and Response Relevance (RR).

Instructions:

1. Success Rate (SR): Assess whether the dialogue successfully completes the predefined task. Identify if all necessary steps were followed and if the task objective was fully achieved. Return 1 for successful completion, 0 for incomplete.

2. Response Relevance (RR): Evaluate the relevance of each system response in relation to the user's queries and the task requirements. Determine the percentage of responses that are contextually appropriate and contribute effectively to task completion. Provide a relevance score as a percentage (0-100).

Please return JSON in the following format without any other text:
{
    "success_rate": {"score": 1/0, "reason": "Brief explanation"},
    "response_relevance": {"score": 0.xxx, "reason": "Brief explanation"}
}"""

        self.prompt2 = """Role:
You are an expert evaluator for task-oriented dialogue systems, responsible for assessing the adherence of dialogues to predefined flows.

Task:
Evaluate the following dialogue to determine the Adherence to Dialogue Flow (ADF).

Instructions:
1. Adherence to Dialogue Flow (ADF): Assess how well the dialogue follows the predefined flow. Consider the logical sequence of interactions, the maintenance of structured progression, and the system's ability to stay on track with the intended dialogue path.

Scoring Criteria:
- Score 5: The dialogue consistently follows the predefined flow with exceptional precision, exhibiting no deviations. Each interaction is seamlessly integrated and directly enhances task progression.
- Score 4: The dialogue adheres closely to the flow, with only minimal deviations that have negligible impact on the task's progression.
- Score 3: The dialogue generally maintains adherence to the flow, with a few minor deviations that slightly affect task progression but do not disrupt the overall sequence.
- Score 2: The dialogue somewhat follows the flow, with noticeable deviations that impact task progression and disrupt the logical sequence to a moderate extent.
- Score 1: The dialogue struggles to adhere to the predefined flow, with significant deviations that hinder task completion and disrupt the logical sequence.

Please return JSON in the following format without any other text:
{
    "ADF": {"score": "1/2/3/4/5", "reason": "Brief explanation"}
}"""

        self.prompt3 = """Role:
You are an expert evaluator for task-oriented dialogue systems, responsible for assessing the control of topic boundaries within dialogues.

Task:
Evaluate the following dialogue to determine the Topic Boundary Control (TBC).

Instructions:
1. Topic Boundary Control (TBC): Assess how accurately the dialogue maintains control over the topic boundaries as defined by the system prompt. Consider the system's ability to stay focused on the intended topic and avoid unnecessary deviations or tangents.

Scoring Criteria:
- Score 5: The dialogue maintains exceptional control over topic boundaries, staying precisely within the defined limits with no deviations. The focus remains consistently on the intended topic throughout.
- Score 4: The dialogue demonstrates strong control over the topic boundaries, with only minor deviations that have little to no impact on the overall focus.
- Score 3: The dialogue generally maintains control over the topic boundaries, with occasional deviations that slightly affect the focus but do not significantly disrupt the topic.
- Score 2: The dialogue shows moderate control over the topic boundaries, with noticeable deviations that impact the focus and disrupt the intended topic to some extent.
- Score 1: The dialogue struggles to maintain control over the topic boundaries, with significant deviations that lead to a loss of focus and disrupt the intended topic.

Please return JSON in the following format without any other text:
{
    "TBC": {"score": "1/2/3/4/5", "reason": "Brief explanation"}
}"""
    
    def evaluate_dialogue(self, dialogue_history):
        """
        Evaluate the dialogue using all three prompts
        """
        try:
            # Evaluate using each prompt
            result1 = self._evaluate_with_prompt(dialogue_history, self.prompt1)
            result2 = self._evaluate_with_prompt(dialogue_history, self.prompt2)
            result3 = self._evaluate_with_prompt(dialogue_history, self.prompt3)
            
            # Combine all results
            combined_results = {
                "metrics_set_1": result1,
                "metrics_set_2": result2,
                "metrics_set_3": result3
            }
            
            return combined_results
            
        except Exception as e:
            print(f"Evaluation error: {e}")
            return self._get_default_evaluation()
    
    def _evaluate_with_prompt(self, dialogue_history, prompt):
        """
        Evaluate dialogue with a specific prompt
        """
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Please evaluate the following dialogue:\n{json.dumps(dialogue_history, ensure_ascii=False, indent=2)}"}
        ]
        
        try:
            result = self.client.chat.completions.create(
                model="gpt-4o-2024-08-06",
                messages=messages,
                stream=False,
                temperature=0.7
            )
            evaluation = result.choices[0].message.content.strip()
            
            try:
                return json.loads(evaluation)
            except json.JSONDecodeError:
                import re
                json_match = re.search(r'\{.*\}', evaluation, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                raise
                
        except Exception as e:
            print(f"Error evaluating with prompt: {e}")
            return {}
    
    def _get_default_evaluation(self):
        """
        Return default evaluation results when evaluation fails
        """
        return {
            "metrics_set_1": {
                "success_rate": {"score": 0, "reason": "Evaluation failed"},
                "response_relevance": {"score": 0.0, "reason": "Evaluation failed"}
            },
            "metrics_set_2": {
                "ADF": {"score": "1", "reason": "Evaluation failed"}
            },
            "metrics_set_3": {
                "TBC": {"score": "1", "reason": "Evaluation failed"}
            }
        }
    
    def calculate_average_scores(self, all_evaluations):
        """
        Calculate average scores across all evaluations
        """
        total_dialogues = len(all_evaluations)
        if total_dialogues == 0:
            return self._get_default_evaluation()
        
        # Initialize score accumulators
        scores = {
            "success_rate": {"total": 0},
            "response_relevance": {"total": 0.0},
            "ADF": {"total": 0.0},
            "TBC": {"total": 0.0}
        }
        
        # Accumulate scores
        for eval_data in all_evaluations:
            evaluation = eval_data["evaluation"]
            
            # Metrics Set 1
            if "success_rate" in evaluation["metrics_set_1"]:
                scores["success_rate"]["total"] += int(evaluation["metrics_set_1"]["success_rate"]["score"])
            
            if "response_relevance" in evaluation["metrics_set_1"]:
                scores["response_relevance"]["total"] += float(evaluation["metrics_set_1"]["response_relevance"]["score"])
            
            # Metrics Set 2
            if "ADF" in evaluation["metrics_set_2"]:
                scores["ADF"]["total"] += float(evaluation["metrics_set_2"]["ADF"]["score"])
            
            # Metrics Set 3
            if "TBC" in evaluation["metrics_set_3"]:
                scores["TBC"]["total"] += float(evaluation["metrics_set_3"]["TBC"]["score"])
        
        # Calculate averages
        return {
            "success_rate": f"{(scores['success_rate']['total'] / total_dialogues):.4f}",
            "response_relevance": f"{scores['response_relevance']['total'] / total_dialogues:.4f}",
            "ADF": f"{scores['ADF']['total'] / total_dialogues:.4f}",
            "TBC": f"{scores['TBC']['total'] / total_dialogues:.4f}"
        }
    
    def save_evaluation_results(self, all_evaluations, output_file="all_evaluations.json"):
        """
        Save evaluation results to a JSON file
        """
        average_scores = self.calculate_average_scores(all_evaluations)
        
        output = {
            "individual_evaluations": all_evaluations,
            "average_scores": average_scores
        }
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        return average_scores 